{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Physics GNN + RL System\n",
    "- Refining the architecture\n",
    "- Simple Physics element\n",
    "- CAUTION: Still in dev - some code block may not make too much sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import networkx as nx\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Batch\n",
    "from torch_geometric.data import Dataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.utils import from_networkx\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1' # Verbose Cuda Error stuff\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE Visuals...NOT IMPORTANT\n",
    "# import matplotlib.pyplot as plt\n",
    "# # Draw edges with thickness based on flow capacity\n",
    "# DG = nx.DiGraph()\n",
    "# # Add edges with attributes\n",
    "# for index, row in df_edges.iterrows():\n",
    "#     DG.add_edge(row['source_node'], row['target_node'], flow_capacity=row['flow_capacity'])\n",
    "\n",
    "# largest_component = max(nx.connected_components(G), key=len)\n",
    "# H = G.subgraph(largest_component).copy()\n",
    "\n",
    "# pos = nx.kamada_kawai_layout(H)\n",
    "\n",
    "# # Edge widths based on the flow capacity attribute\n",
    "# edge_widths = [H[u][v]['flow_capacity']*1.5 for u, v in H.edges()]\n",
    "\n",
    "# plt.figure(figsize=(12, 12))\n",
    "# nx.draw_networkx_nodes(H, pos, node_size=700, node_color='skyblue', alpha=0.6)\n",
    "# edges = nx.draw_networkx_edges(H, pos, edge_color='blue', width=edge_widths, alpha=0.7, arrows=True)\n",
    "# nx.draw_networkx_labels(H, pos, font_size=14, font_color='darkblue')\n",
    "\n",
    "# plt.title('Synthetic Network with Flow Capacities (Thickness)')\n",
    "# plt.axis('off')  # Turn off the axis\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Simple GNN Architecture\n",
    "$$\n",
    "h_v^{(k+1)} = \\sigma \\left( \\sum_{u \\in \\mathcal{N}(v)} \\frac{1}{\\sqrt{d_u d_v}} W^{(k)} h_u^{(k)} \\right)\n",
    "$$\n",
    "\n",
    "where $ h_v^{(k)} $ represents the node features at layer $ k $, and $ W^{(k)} $ is the weight matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE GNN Simple Architecture\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, out_channels)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.3, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.3, training=self.training)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reinforcement Learning (Policy Network)\n",
    "The policy network in the RL model maps the state to an action:\n",
    "\n",
    "$$\n",
    "\\text{Action} = \\text{PolicyNetwork}(State)\n",
    "$$\n",
    "\n",
    "Optimization Methods\n",
    "- Switching Optimizer\n",
    "- Hierarchical RL\n",
    "- Aggergator Method (Could Try but from theory calculation it's not good)\n",
    "\n",
    "Adding Physics\n",
    "- Conservation of Mass\n",
    "- Darcy-Weisbach (Flow Rate) | https://en.wikipedia.org/wiki/Darcy%E2%80%93Weisbach_equation\n",
    "- Junctions and Flow Splitting\n",
    "- Pressure and Velocity\n",
    "\n",
    "Exploration Strat\n",
    "- Entropy Regularization\n",
    "- Epsilon-Greedy Policies\n",
    "\n",
    "Rewards Shape\n",
    "- Overflow Penalty: Penalize overflows to prioritize their prevention\n",
    "- Stability Reward: Encourage maintaing water levels close to an ideal value\n",
    "- Energy Efficiency: Adding Penalties for excessive valve adjustments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return torch.sigmoid(self.fc3(x)).squeeze(-1)  # Constrain actions between 0 and 1\n",
    "\n",
    "# Local control: Adjust valves (non-junction nodes)\n",
    "def local_control(node_representations, policy_network, junction_nodes, last_adjustments, current_time, adjustment_interval=15):\n",
    "    non_junction_mask = torch.ones(node_representations.shape[0], dtype=torch.bool, device=node_representations.device)\n",
    "    non_junction_mask[junction_nodes] = False\n",
    "    non_junction_representations = node_representations[non_junction_mask]\n",
    "    \n",
    "    adjustable = torch.tensor(\n",
    "        [(current_time - last_adjustments[node]) >= adjustment_interval for node in range(num_nodes)],\n",
    "        dtype=torch.bool,\n",
    "        device=node_representations.device\n",
    "    )\n",
    "    adjustable = adjustable[non_junction_mask]\n",
    "    \n",
    "    actions = torch.zeros(non_junction_representations.shape[0], device=node_representations.device)\n",
    "    if adjustable.sum() > 0:\n",
    "        adjustable_reps = non_junction_representations[adjustable]\n",
    "        adjustable_actions = policy_network(adjustable_reps)\n",
    "        actions[adjustable] = adjustable_actions\n",
    "    \n",
    "    # Create full action vector\n",
    "    full_actions = torch.zeros(node_representations.shape[0], device=node_representations.device)\n",
    "    full_actions[non_junction_mask] = actions\n",
    "    return full_actions  # Shape [num_nodes]\n",
    "\n",
    "# Global control: Adjust all valves based on global state\n",
    "def global_control(node_representations, policy_network):\n",
    "    graph_representation = torch.mean(node_representations, dim=0)\n",
    "    global_action = policy_network(graph_representation.unsqueeze(0))\n",
    "    actions = global_action.expand(node_representations.shape[0], -1)\n",
    "    return actions\n",
    "\n",
    "# Hybrid control: Switch between local and global control based on water levels\n",
    "def hybrid_control(node_representations, policy_network, global_threshold, current_water_levels, junction_nodes, last_adjustments, current_time, adjustment_interval=15):\n",
    "    max_water_level = torch.max(current_water_levels)\n",
    "    if max_water_level > global_threshold:\n",
    "        print(\"Switching to Global Control!\")\n",
    "        actions = global_control(node_representations, policy_network)\n",
    "    else:\n",
    "        print(\"Using Local Control.\")\n",
    "        actions = local_control(node_representations, policy_network, junction_nodes, last_adjustments, current_time, adjustment_interval)\n",
    "    return actions\n",
    "\n",
    "# Darcy-Weisbach flow calculation\n",
    "def darcy_weisbach_flow(src_level, tgt_level, distance, friction_factor):\n",
    "    pressure_diff = src_level - tgt_level\n",
    "    if pressure_diff <= 0:\n",
    "        return torch.tensor(0.0, device=distance.device)\n",
    "    flow_rate = pressure_diff / (friction_factor * distance)\n",
    "    return flow_rate\n",
    "\n",
    "# Update state based on actions and compute rewards\n",
    "def environment_step(state, action, edge_index, flow_capacities, junction_nodes, splitting_ratios, friction_factors, last_adjustments, current_time, adjustment_interval=15):\n",
    "    water_levels = state[:, 0].clone()\n",
    "    inflow_rate = state[:, 1].clone()\n",
    "    outflow_rate = state[:, 2].clone()\n",
    "    valve_position = action\n",
    "    inflow_rate_new = torch.zeros_like(inflow_rate)\n",
    "\n",
    "    for edge_idx, (src, tgt) in enumerate(edge_index.t()):\n",
    "        src, tgt = int(src.item()), int(tgt.item())\n",
    "        if src in junction_nodes or tgt in junction_nodes:\n",
    "            junction = src if src in junction_nodes else tgt\n",
    "            if junction in splitting_ratios:\n",
    "                for downstream_node, ratio in splitting_ratios[junction].items():\n",
    "                    split_flow = inflow_rate[src] * ratio\n",
    "                    head_loss = friction_factors[edge_idx] * split_flow\n",
    "                    flow = torch.clamp(split_flow - head_loss, min=0.0) # Non Negative!!\n",
    "                    assert flow.dim() == 0, f\"Flow tensor at edge {edge_idx} is not scalar: {flow.shape}\"\n",
    "                    inflow_rate_new[downstream_node] += flow\n",
    "            else:\n",
    "                print(f\"Junction node {junction} has no splitting ratios assigned.\")\n",
    "        else:\n",
    "            flow_rate = darcy_weisbach_flow(water_levels[src], water_levels[tgt], distances_tensor[edge_idx], friction_factors[edge_idx])\n",
    "            flow_rate = flow_rate * valve_position[src]\n",
    "            flow_rate = flow_rate.squeeze()\n",
    "            assert flow_rate.dim() == 0, f\"flow_rate tensor at edge {edge_idx} is not scalar: {flow_rate.shape}\"\n",
    "            inflow_rate_new[tgt] += flow_rate\n",
    "    assert inflow_rate_new.dim() == 1, f\"inflow_rate_new has incorrect shape: {inflow_rate_new.shape}\"\n",
    "\n",
    "    new_water_level = torch.clamp(water_levels + inflow_rate_new - outflow_rate, 0, None)\n",
    "    new_inflow_rate = torch.clamp(inflow_rate_new, 0, None)\n",
    "    new_outflow_rate = outflow_rate\n",
    "    new_valve_position = valve_position.clone()\n",
    "\n",
    "    # Update last valve adjustment times (assuming 15 mins)\n",
    "    adjusted_nodes = (action != state[:, 3]).nonzero(as_tuple=True)[0]\n",
    "    for node in adjusted_nodes:\n",
    "        last_adjustments[node] = current_time\n",
    "\n",
    "    new_state = torch.stack((new_water_level, new_inflow_rate, new_outflow_rate, new_valve_position), dim=1)\n",
    "    reward = overflow_penalty(new_state) + stability_reward(new_state)\n",
    "    return new_state, reward\n",
    "\n",
    "# Reward functions\n",
    "def overflow_penalty(state):\n",
    "    penalty = torch.clamp(state[:, 0] - 1.5, min=0) # Exceeding 1.5 units\n",
    "    return -torch.sum(penalty) * 10  # Higher penalty for overflow\n",
    "\n",
    "def stability_reward(state):\n",
    "    ideal_level = 1.0 # Reward for maintaining water levels = 1.0\n",
    "    return -torch.mean((state[:, 0] - ideal_level) ** 2)  # Minimize deviation from the ideal level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next Steps (Might be bottlenecked by computational resources)\n",
    "- MARL\n",
    "- Advanced Reward System\n",
    "- Testing different learning strats (PPO vs Q-Learning vs A2C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Synthetic Data\n",
    "- <b>water_level</b>: The current water level at each node at each timestep. It is updated based on inflows (both natural and from neighboring nodes) and outflows (controlled by the valve).\n",
    "- <b>inflow_rate</b>: The natural inflow rate into the node, modulated by a seasonal/cyclic pattern.\n",
    "- <b>outflow_rate</b>: The outflow rate is a function of the valve position and the node's current water level.\n",
    "- <b>valve_position</b>: The valve position for each node, controlling how much water flows out. It can be controlled by your policy network.\n",
    "- Cross Section Area\n",
    "- Gradient\n",
    "- Junctions - Multi Inflow / One out flow\n",
    "- Catachment Runoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 50\n",
    "num_edges = 60  # Average 2.4 edges per node\n",
    "num_junctions = 10\n",
    "time_steps = 150\n",
    "seasonal_cycle_length = 1440  # One day in minutes (assuming 1 min)\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "def generate_directed_graph(n_nodes, n_edges, seed=None):\n",
    "    while True:\n",
    "        G = nx.gnm_random_graph(n_nodes, n_edges, seed=seed, directed=True)\n",
    "        if nx.is_weakly_connected(G):\n",
    "            return G\n",
    "G = generate_directed_graph(num_nodes, num_edges, seed=42)\n",
    "print(f\"Generated directed graph with {num_nodes} nodes and {G.number_of_edges()} edges.\")\n",
    "\n",
    "# Edges: Flow capacities, distances, and friction factors\n",
    "flow_capacities = np.random.uniform(low=1.0, high=3.0, size=(G.number_of_edges(),))\n",
    "distances = np.random.uniform(low=0.5, high=5.0, size=(G.number_of_edges(),))\n",
    "friction_factors = np.random.uniform(low=0.01, high=0.1, size=(G.number_of_edges(),))\n",
    "for idx, (u, v) in enumerate(G.edges()):\n",
    "    G[u][v]['flow_capacity'] = flow_capacities[idx]\n",
    "    G[u][v]['distance'] = distances[idx]\n",
    "    G[u][v]['friction_factor'] = friction_factors[idx]\n",
    "print(\"Assigned edge attributes to all edges.\")\n",
    "\n",
    "possible_junction_nodes = [node for node in G.nodes() if G.out_degree(node) > 0]\n",
    "if len(possible_junction_nodes) < num_junctions:\n",
    "    raise ValueError(f\"Not enough nodes with outgoing edges to select {num_junctions} junction nodes.\")\n",
    "junction_nodes = random.sample(possible_junction_nodes, num_junctions)\n",
    "print(f\"Selected junction nodes: {junction_nodes}\")\n",
    "\n",
    "# Define splitting ratios for junction nodes\n",
    "splitting_ratios = {}\n",
    "head_loss_factor = 0.1  # Energy loss when splitting\n",
    "\n",
    "for node in junction_nodes:\n",
    "    # Get outgoing edges for the junction node\n",
    "    downstream_nodes = list(G.successors(node))\n",
    "    num_downstream = len(downstream_nodes)\n",
    "    if num_downstream > 0:\n",
    "        raw_ratios = np.random.uniform(0.1, 1.0, size=num_downstream)\n",
    "        normalized_ratios = raw_ratios / np.sum(raw_ratios)\n",
    "        splitting_ratios[node] = dict(zip(downstream_nodes, normalized_ratios))\n",
    "        print(f\"Splitting ratios for node {node}: {splitting_ratios[node]}\")\n",
    "    else:\n",
    "        print(f\"Junction node {node} has no downstream nodes assigned.\")\n",
    "        # Optionally, assign default splitting ratios or handle accordingly\n",
    "\n",
    "# Simulating rainfall with a sinusoidal pattern\n",
    "time = np.arange(time_steps)\n",
    "seasonal_pattern = np.sin(2 * np.pi * time / seasonal_cycle_length) * 0.2 + 0.3\n",
    "\n",
    "# Initialize water levels, inflow/outflow rates, and valve positions\n",
    "initial_water_levels = np.random.uniform(low=0.5, high=2.0, size=num_nodes)\n",
    "water_levels = np.zeros((time_steps, num_nodes))\n",
    "inflow_rates = np.zeros((time_steps, num_nodes))\n",
    "outflow_rates = np.zeros((time_steps, num_nodes))\n",
    "valve_positions = np.zeros((time_steps, num_nodes))\n",
    "\n",
    "# Set initial conditions\n",
    "water_levels[0, :] = initial_water_levels\n",
    "valve_positions[0, :] = np.random.uniform(low=0.0, high=1.0, size=num_nodes)\n",
    "print(f\"Initial water levels: {initial_water_levels}\")\n",
    "print(f\"Initial valve positions: {valve_positions[0]}\")\n",
    "\n",
    "# Physics parameters\n",
    "friction_coefficients = 10 * np.ones(G.number_of_edges())  # Adjust as per system\n",
    "hydrostatic_heads = distances.copy()  # Pressure of flow\n",
    "\n",
    "# Convert NetworkX directed graph to PyTorch Geometric Data object\n",
    "water_data = from_networkx(G)\n",
    "print(\"Converted directed NetworkX graph to PyTorch Geometric Data object.\")\n",
    "\n",
    "# Assign edge attributes\n",
    "flow_capacities_tensor = torch.tensor(flow_capacities, dtype=torch.float).to(device)\n",
    "distances_tensor = torch.tensor(distances, dtype=torch.float).to(device)\n",
    "friction_factors_tensor = torch.tensor(friction_factors, dtype=torch.float).to(device)\n",
    "\n",
    "water_data.flow_capacity = flow_capacities_tensor\n",
    "water_data.distance = distances_tensor\n",
    "water_data.friction_factor = friction_factors_tensor\n",
    "\n",
    "print(\"Assigned edge attributes to water_data.\")\n",
    "\n",
    "# Verify that edge attributes are assigned\n",
    "assert hasattr(water_data, 'flow_capacity'), \"flow_capacity not found in water_data.\"\n",
    "assert hasattr(water_data, 'distance'), \"distance not found in water_data.\"\n",
    "assert hasattr(water_data, 'friction_factor'), \"friction_factor not found in water_data.\"\n",
    "print(\"All required edge attributes are present in water_data.\")\n",
    "\n",
    "# Verify that the number of edge attributes matches edge_index\n",
    "num_edges_data = water_data.edge_index.shape[1]\n",
    "print(f\"Number of edges in water_data.edge_index: {num_edges_data}\")\n",
    "print(f\"Length of flow_capacities_tensor: {flow_capacities_tensor.shape[0]}\")\n",
    "print(f\"Length of distances_tensor: {distances_tensor.shape[0]}\")\n",
    "print(f\"Length of friction_factors_tensor: {friction_factors_tensor.shape[0]}\")\n",
    "\n",
    "assert num_edges_data == flow_capacities_tensor.shape[0], \"Mismatch between edge_index and flow_capacities_tensor\"\n",
    "assert num_edges_data == distances_tensor.shape[0], \"Mismatch between edge_index and distances_tensor\"\n",
    "assert num_edges_data == friction_factors_tensor.shape[0], \"Mismatch between edge_index and friction_factors_tensor\"\n",
    "print(\"All edge attribute tensors match the number of edges in edge_index.\")\n",
    "\n",
    "# Move edge_index to device\n",
    "edge_index = water_data.edge_index.to(device)\n",
    "\n",
    "# Split data into training, validation, and test sets\n",
    "train_steps = int(0.7 * time_steps)\n",
    "val_steps = int(0.15 * time_steps)\n",
    "test_steps = time_steps - train_steps - val_steps\n",
    "\n",
    "# Prepare node features as tensors\n",
    "gnn_in_chan = 4  # water_level, inflow_rate, outflow_rate, valve_position\n",
    "node_features = np.stack([\n",
    "    water_levels,\n",
    "    inflow_rates,\n",
    "    outflow_rates,\n",
    "    valve_positions\n",
    "], axis=2)  # Shape: [time_steps, num_nodes, 4]\n",
    "\n",
    "node_features_tensor = torch.tensor(node_features, dtype=torch.float).to(device)\n",
    "\n",
    "# Split node features\n",
    "train_node_features = node_features_tensor[:train_steps, :, :]\n",
    "val_node_features = node_features_tensor[train_steps:train_steps+val_steps, :, :]\n",
    "test_node_features = node_features_tensor[train_steps+val_steps:, :, :]\n",
    "\n",
    "# Create PyTorch Geometric Data objects for each time step\n",
    "train_data = [Data(x=train_node_features[t], edge_index=edge_index) for t in range(train_steps)]\n",
    "val_data = [Data(x=val_node_features[t], edge_index=edge_index) for t in range(val_steps)]\n",
    "test_data = [Data(x=test_node_features[t], edge_index=edge_index) for t in range(test_steps)]\n",
    "print(\"Prepared training, validation, and test datasets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NOTE Simple but effective way to add X features (Could improve in later phases)\n",
    "# gnn_in_chan = 4  # aka. Num Input Feat (e.g, water_level, inflow_rate, outflow_rate, valve_position)\n",
    "# node_features = np.zeros((time_steps, num_nodes, gnn_in_chan))\n",
    "# for t in range(time_steps):\n",
    "#     for n in range(num_nodes):\n",
    "#         node_features[t, n, 0] = df_nodes.loc[(df_nodes['time_step'] == t) & (df_nodes['node'] == n), 'water_level'].values[0]\n",
    "#         node_features[t, n, 1] = df_nodes.loc[(df_nodes['time_step'] == t) & (df_nodes['node'] == n), 'inflow_rate'].values[0]\n",
    "#         node_features[t, n, 2] = df_nodes.loc[(df_nodes['time_step'] == t) & (df_nodes['node'] == n), 'outflow_rate'].values[0]\n",
    "#         node_features[t, n, 3] = df_nodes.loc[(df_nodes['time_step'] == t) & (df_nodes['node'] == n), 'valve_position'].values[0]\n",
    "# node_features_tensor = torch.tensor(node_features, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn_in_chan = 4  # water_level, inflow_rate, outflow_rate, valve_position\n",
    "node_features = np.stack([\n",
    "    water_levels,\n",
    "    inflow_rates,\n",
    "    outflow_rates,\n",
    "    valve_positions\n",
    "], axis=2)  # Shape: [time_steps, num_nodes, 4]\n",
    "\n",
    "node_features_tensor = torch.tensor(node_features, dtype=torch.float).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE Will need LSTM for multi-step to deal with sequencial data: node_features_tensor.to(device)\n",
    "# single_immediate_features = node_features[0] # NOTE for immediate condition (Snapshot)\n",
    "# single_immediate_features_tensor = torch.tensor(single_immediate_features, dtype=torch.float).to(device)\n",
    "# print(f\"Node Features: {node_features.shape}\")\n",
    "# print(f\"Single Node Features: {single_immediate_features.shape}\")\n",
    "\n",
    "# water_data = from_networkx(G)\n",
    "# water_data.x = single_immediate_features_tensor\n",
    "# water_data.edge_index = water_data.edge_index.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edge attributes successfully assigned to water_data.\n",
      "Number of edges in edge_index: 120\n",
      "Length of flow_capacities_tensor: 60\n",
      "Length of distances_tensor: 60\n",
      "Length of friction_factors_tensor: 60\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Mismatch between edge_index and flow_capacities_tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength of friction_factors_tensor: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfriction_factors_tensor\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Assert that all edge attribute tensors have the same length as edge_index\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m edge_index\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m flow_capacities_tensor\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMismatch between edge_index and flow_capacities_tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m edge_index\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m distances_tensor\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMismatch between edge_index and distances_tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m edge_index\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m friction_factors_tensor\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMismatch between edge_index and friction_factors_tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mAssertionError\u001b[0m: Mismatch between edge_index and flow_capacities_tensor"
     ]
    }
   ],
   "source": [
    "# Extract edge attributes from NetworkX graph\n",
    "flow_capacities = [G[u][v]['flow_capacity'] for u, v in G.edges()]\n",
    "distances = [G[u][v]['distance'] for u, v in G.edges()]\n",
    "friction_factors = [G[u][v]['friction_factor'] for u, v in G.edges()]\n",
    "\n",
    "# Convert lists to PyTorch tensors and move to the appropriate device\n",
    "flow_capacities_tensor = torch.tensor(flow_capacities, dtype=torch.float).to(device)\n",
    "distances_tensor = torch.tensor(distances, dtype=torch.float).to(device)\n",
    "friction_factors_tensor = torch.tensor(friction_factors, dtype=torch.float).to(device)\n",
    "\n",
    "# Assign edge attributes to the Data object\n",
    "water_data.flow_capacity = flow_capacities_tensor\n",
    "water_data.distance = distances_tensor\n",
    "water_data.friction_factor = friction_factors_tensor\n",
    "\n",
    "# Verify that edge attributes are assigned\n",
    "assert hasattr(water_data, 'flow_capacity'), \"flow_capacity not found in water_data.\"\n",
    "assert hasattr(water_data, 'distance'), \"distance not found in water_data.\"\n",
    "assert hasattr(water_data, 'friction_factor'), \"friction_factor not found in water_data.\"\n",
    "\n",
    "print(\"Edge attributes successfully assigned to water_data.\")\n",
    "\n",
    "edge_index = water_data.edge_index.to(device)\n",
    "# After assigning edge attributes and before the training loop\n",
    "print(f\"Number of edges in edge_index: {edge_index.shape[1]}\")\n",
    "print(f\"Length of flow_capacities_tensor: {flow_capacities_tensor.shape[0]}\")\n",
    "print(f\"Length of distances_tensor: {distances_tensor.shape[0]}\")\n",
    "print(f\"Length of friction_factors_tensor: {friction_factors_tensor.shape[0]}\")\n",
    "\n",
    "# Assert that all edge attribute tensors have the same length as edge_index\n",
    "assert edge_index.shape[1] == flow_capacities_tensor.shape[0], \"Mismatch between edge_index and flow_capacities_tensor\"\n",
    "assert edge_index.shape[1] == distances_tensor.shape[0], \"Mismatch between edge_index and distances_tensor\"\n",
    "assert edge_index.shape[1] == friction_factors_tensor.shape[0], \"Mismatch between edge_index and friction_factors_tensor\"\n",
    "print(\"All edge attribute tensors match the number of edges in edge_index.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training, validation, and test sets\n",
    "train_steps = int(0.7 * time_steps)\n",
    "val_steps = int(0.15 * time_steps)\n",
    "test_steps = time_steps - train_steps - val_steps\n",
    "\n",
    "train_node_features = node_features_tensor[:train_steps, :, :]\n",
    "val_node_features = node_features_tensor[train_steps:train_steps+val_steps, :, :]\n",
    "test_node_features = node_features_tensor[train_steps+val_steps:, :, :]\n",
    "\n",
    "train_data = [Data(x=train_node_features[t], edge_index=edge_index) for t in range(train_steps)]\n",
    "val_data = [Data(x=val_node_features[t], edge_index=edge_index) for t in range(val_steps)]\n",
    "test_data = [Data(x=test_node_features[t], edge_index=edge_index) for t in range(test_steps)]\n",
    "\n",
    "# Initialize friction factors (if not already done)\n",
    "actual_num_edges = edge_index.shape[1]\n",
    "friction_factors = np.random.uniform(low=0.01, high=0.1, size=(actual_num_edges,))\n",
    "friction_factors_tensor = torch.tensor(friction_factors, dtype=torch.float).to(device)\n",
    "assert friction_factors_tensor.shape[0] == edge_index.shape[1], \"Mismatch in number of edges and friction factors\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switching to Global Control!\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 60 is out of bounds for dimension 0 with size 60",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 72\u001b[0m\n\u001b[0;32m     60\u001b[0m actions \u001b[38;5;241m=\u001b[39m hybrid_control(\n\u001b[0;32m     61\u001b[0m     node_representations, \n\u001b[0;32m     62\u001b[0m     policy_network, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     68\u001b[0m     adjustment_interval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m\n\u001b[0;32m     69\u001b[0m )\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# Environment step: simulate water dynamics and calculate reward\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m new_state, reward \u001b[38;5;241m=\u001b[39m \u001b[43menvironment_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflow_capacities_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjunction_nodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplitting_ratios\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfriction_factors_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlast_valve_adjustment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43madjustment_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\n\u001b[0;32m     83\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m# Update current state\u001b[39;00m\n\u001b[0;32m     86\u001b[0m current_water_levels \u001b[38;5;241m=\u001b[39m new_state[:, \u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "Cell \u001b[1;32mIn[36], line 93\u001b[0m, in \u001b[0;36menvironment_step\u001b[1;34m(state, action, edge_index, flow_capacities, junction_nodes, splitting_ratios, friction_factors, last_adjustments, current_time, adjustment_interval)\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJunction node \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjunction\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has no splitting ratios assigned.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;66;03m# Calculate flow using Darcy-Weisbach\u001b[39;00m\n\u001b[1;32m---> 93\u001b[0m     flow_rate \u001b[38;5;241m=\u001b[39m darcy_weisbach_flow(water_levels[src], water_levels[tgt], \u001b[43mdistances_tensor\u001b[49m\u001b[43m[\u001b[49m\u001b[43medge_idx\u001b[49m\u001b[43m]\u001b[49m, friction_factors[edge_idx])\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;66;03m# Apply valve position control\u001b[39;00m\n\u001b[0;32m     95\u001b[0m     flow_rate \u001b[38;5;241m=\u001b[39m flow_rate \u001b[38;5;241m*\u001b[39m valve_position[src]\n",
      "\u001b[1;31mIndexError\u001b[0m: index 60 is out of bounds for dimension 0 with size 60"
     ]
    }
   ],
   "source": [
    "# NOTE New Training Method\n",
    "gnn_out_chann = gnn_in_chan # Num of features in = out\n",
    "rl_out_channels = 1 # Controlling valve positions (one output per node)\n",
    "hidden_channels = 128\n",
    "gnn = GCN(in_channels=gnn_in_chan, hidden_channels=hidden_channels, out_channels=gnn_out_chann).to(device)\n",
    "policy_network = PolicyNetwork(input_dim=gnn_out_chann, output_dim=rl_out_channels).to(device)\n",
    "optimizer = torch.optim.Adam(list(gnn.parameters()) + list(policy_network.parameters()), lr=1e-3)\n",
    "# schedule = ReduceLROnPlateau\n",
    "\n",
    "num_episodes = 1_000\n",
    "best_val_loss = float(\"inf\")\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "es_threshold = 15\n",
    "early_stoppping = 0\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True) # Inplace Error??\n",
    "# Training Loop\n",
    "for episode in range(num_episodes):\n",
    "    gnn.train()\n",
    "    policy_network.train()\n",
    "    total_train_loss = 0\n",
    "\n",
    "    # Initialize episode-specific state\n",
    "    current_water_levels = initial_water_levels.copy()\n",
    "    current_inflow_rates = np.zeros(num_nodes)\n",
    "    current_outflow_rates = np.zeros(num_nodes)\n",
    "    current_valve_positions = valve_positions[0].copy()\n",
    "\n",
    "    # Reset last valve adjustment tracker\n",
    "    last_valve_adjustment = np.full(num_nodes, -np.inf, dtype=np.float32)\n",
    "\n",
    "    # Iterate through time steps within the episode\n",
    "    for t in range(time_steps):\n",
    "        # Prepare current state tensor\n",
    "        state = {\n",
    "            'water_level': current_water_levels,\n",
    "            'inflow_rate': current_inflow_rates,\n",
    "            'outflow_rate': current_outflow_rates,\n",
    "            'valve_position': current_valve_positions,\n",
    "        }\n",
    "        state_tensor = torch.tensor([\n",
    "            state['water_level'],\n",
    "            state['inflow_rate'],\n",
    "            state['outflow_rate'],\n",
    "            state['valve_position']\n",
    "        ], dtype=torch.float).transpose(0, 1).to(device)  # Shape: [num_nodes, 4]\n",
    "\n",
    "        data = Data(x=state_tensor, edge_index=edge_index)\n",
    "        data = data.to(device)\n",
    "\n",
    "        # GNN forward pass\n",
    "        node_representations = gnn(data)\n",
    "\n",
    "        # Determine global threshold (95th percentile of current water levels)\n",
    "        train_current_water_levels = torch.tensor(current_water_levels, dtype=torch.float).to(device)\n",
    "        train_global_threshold = torch.quantile(train_current_water_levels, 0.95)\n",
    "\n",
    "        # Hybrid control: decide between local and global control\n",
    "        actions = hybrid_control(\n",
    "            node_representations, \n",
    "            policy_network, \n",
    "            train_global_threshold, \n",
    "            train_current_water_levels, \n",
    "            torch.tensor(junction_nodes, device=device), \n",
    "            torch.tensor(last_valve_adjustment, device=device), \n",
    "            t, \n",
    "            adjustment_interval=15\n",
    "        )\n",
    "\n",
    "        # Environment step: simulate water dynamics and calculate reward\n",
    "        new_state, reward = environment_step(\n",
    "            state_tensor, \n",
    "            actions, \n",
    "            edge_index,\n",
    "            flow_capacities_tensor, \n",
    "            junction_nodes, \n",
    "            splitting_ratios,\n",
    "            friction_factors_tensor,\n",
    "            last_valve_adjustment,\n",
    "            t,\n",
    "            adjustment_interval=15\n",
    "        )\n",
    "\n",
    "        # Update current state\n",
    "        current_water_levels = new_state[:, 0].detach().cpu().numpy()\n",
    "        current_inflow_rates = new_state[:, 1].detach().cpu().numpy()\n",
    "        current_outflow_rates = new_state[:, 2].detach().cpu().numpy()\n",
    "        current_valve_positions = new_state[:, 3].detach().cpu().numpy()\n",
    "\n",
    "        # Compute loss (maximize reward -> minimize negative reward)\n",
    "        train_loss = -reward.mean()\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += train_loss.item()\n",
    "\n",
    "    # Average training loss for the episode\n",
    "    average_train_loss = total_train_loss / time_steps\n",
    "    train_losses.append(average_train_loss)\n",
    "\n",
    "    # Validation Phase\n",
    "    gnn.eval()\n",
    "    policy_network.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for t, batch in enumerate(val_data):\n",
    "            batch = batch.to(device)\n",
    "            current_time = train_steps + t\n",
    "\n",
    "            # Extract current water levels for threshold\n",
    "            val_current_water_levels = batch.x[:, 0]\n",
    "            val_global_threshold = torch.quantile(val_current_water_levels, 0.95)\n",
    "\n",
    "            # GNN forward pass\n",
    "            val_node_representations = gnn(batch)\n",
    "\n",
    "            # Hybrid control\n",
    "            val_actions = hybrid_control(\n",
    "                val_node_representations, \n",
    "                policy_network, \n",
    "                val_global_threshold, \n",
    "                val_current_water_levels, \n",
    "                torch.tensor(junction_nodes, device=device), \n",
    "                torch.tensor(last_valve_adjustment, device=device), \n",
    "                current_time, \n",
    "                adjustment_interval=15\n",
    "            )\n",
    "\n",
    "            # Environment step\n",
    "            val_new_state, val_reward = environment_step(\n",
    "                batch.x, \n",
    "                val_actions, \n",
    "                edge_index, \n",
    "                flow_capacities_tensor, \n",
    "                junction_nodes, \n",
    "                splitting_ratios, \n",
    "                friction_factors_tensor,\n",
    "                last_valve_adjustment,\n",
    "                current_time,\n",
    "                adjustment_interval=15\n",
    "            )\n",
    "\n",
    "            # Compute validation loss\n",
    "            val_loss = -val_reward.mean()\n",
    "            total_val_loss += val_loss.item()\n",
    "    average_val_loss = total_val_loss / len(val_data)\n",
    "    val_losses.append(average_val_loss)\n",
    "\n",
    "    # Early Stopping and Model Saving\n",
    "    if average_val_loss < best_val_loss:\n",
    "        best_val_loss = average_val_loss\n",
    "        early_stopping_counter = 0  # Reset counter\n",
    "        torch.save(gnn.state_dict(), 'best_gnn_model.pth')\n",
    "        torch.save(policy_network.state_dict(), 'best_policy_network.pth')\n",
    "        print(f\"Episode {episode}: New best validation loss {best_val_loss:.4f}. Models saved.\")\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "        print(f\"Episode {episode}: Validation loss did not improve. Counter: {early_stopping_counter}/{es_threshold}\")\n",
    "        if early_stopping_counter >= es_threshold:\n",
    "            print(f\"Early Stopping triggered at Episode {episode}\")\n",
    "            break\n",
    "\n",
    "    print(f\"Episode {episode}: Train Loss = {average_train_loss:.4f}, Val Loss = {average_val_loss:.4f}\")\n",
    "\n",
    "# Load the best models after training\n",
    "gnn.load_state_dict(torch.load('best_gnn_model.pth'))\n",
    "policy_network.load_state_dict(torch.load('best_policy_network.pth'))\n",
    "print(\"Loaded the best-performing models.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GAT\n",
    "In the GAT layer, the node representations are updated using attention mechanisms:\n",
    "\n",
    "$$\n",
    "h_v' = \\sum_{u \\in \\mathcal{N}(v)} \\alpha_{vu} W h_u\n",
    "$$\n",
    "\n",
    "where $ \\alpha_{vu} $ is the attention coefficient between node $ v $ and node $ u $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tnc_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
